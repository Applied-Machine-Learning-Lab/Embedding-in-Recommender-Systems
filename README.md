# Embedding-in-Recommender-Systems (Continuously Updating) [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

This repository is consistent with [our survey paper](https://arxiv.org/pdf/2310.18608.pdf)’ Embedding in Recommender Systems: A Survey. ‘ If you find this work helpful, we would appreciate it if you could cite our paper in the following form:
```
@article{zhao2023embedding,
  title={Embedding in Recommender Systems: A Survey},
  author={Zhao, Xiangyu and Wang, Maolin and Zhao, Xinjian and Li, Jiansheng and Zhou, Shucheng and Yin, Dawei and Li, Qing and Tang, Jiliang and Guo, Ruocheng},
  journal={arXiv preprint arXiv:2310.18608},
  year={2023}
}
```

## Survey
### 2023
1. Li, Shiwei, et al. "Embedding Compression in Recommender Systems: A Survey." ACM Computing Surveys (2023). [link](https://dl.acm.org/doi/10.1145/3637841)
2. Zheng, R., Qu, L., Cui, B., Shi, Y., & Yin, H. (2023). Automl for deep recommender systems: A survey. ACM Transactions on Information Systems, 41(4), 1-38.
 [link](https://arxiv.org/pdf/2203.13922.pdf)
### Before 2023
1. D. kumar Bokde, S. Girase, and D. Mukhopadhyay, “Role of matrix factorization model in collaborative filtering algorithm: A survey. IJAFRC 2015. [link](https://arxiv.org/ftp/arxiv/papers/1503/1503.07475.pdf)
2. M. H. Abdi, G. Okeyo, and R. W. Mwangi, “Matrix factorization techniques for context-aware collaborative filtering recommender systems: A survey,” 2018. [link](https://dora.dmu.ac.uk/server/api/core/bitstreams/a95c914f-f2f8-4cc7-bf5e-78eee41c7036/content)
3. W.-S. Chen, Q. Zeng, and B. Pan, “A survey of deep nonnegative matrix factorization,” Neurocomputing, vol. 491, pp. 305–320, 2022. [link](https://www.sciencedirect.com/science/article/abs/pii/S0925231222003058)
4. A. K. Qin, V. L. Huang, and P. N. Suganthan, “Differential evolution algorithm with strategy adaptation for global numerical optimization,” IEEE transactions on Evolutionary Computation, vol. 13, no. 2, pp. 398–417, 2008.
5. B. Chen, X. Zhao, Y. Wang, W. Fan, H. Guo, and R. Tang, “Automated machine learning for deep recommender systems: A survey,” ArXiv, vol. abs/2204.01390, 2022. [link](https://arxiv.org/pdf/2204.01390.pdf)
6. J. Yu, H. Yin, X. Xia, T. Chen, J. Li, and Z. Huang, “Selfsupervised learning for recommender systems: A survey,” TKDE 2022. [link](https://arxiv.org/pdf/2203.15876.pdf)
7. N. M. Nasrabadi and R. A. King, “Image coding using vector quantization: A review,” IEEE Transactions on communications, vol. 36,no. 8, pp. 957–971, 1988. [link](https://ieeexplore.ieee.org/document/3776)
8. S. T. Ali and M. Englis, “Quantization methods: a guide for physicists and analysts,” Reviews in Mathematical Physics, vol. 17, no. 04, pp. 391–490, 2005. [link](https://arxiv.org/abs/math-ph/0405065)
9. T.-C. Lu and C.-C. Chang, “A survey of vq codebook generation.” J. Inf. Hiding Multim. Signal Process., vol. 1, no. 3, pp. 190–203, 2010. [link](https://bit.kuas.edu.tw/~jihmsp/2010/vol1/JIH-MSP-2010-03-004.pdf)
10. A. Ramanan and M. Niranjan, “A review of codebook models in patchbased visual object recognition,” Journal of Signal Processing Systems, vol. 68, no. 3, pp. 333 352, 2012. [link](https://link.springer.com/article/10.1007/s11265-011-0622-x)
11. N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, and A. Galstyan, “A survey on bias and fairness in machine learning,” ACM Computing Surveys (CSUR), vol. 54, no. 6, pp. 1–35, 2021. [link](https://arxiv.org/abs/1908.09635)

## CF & MF
| Name | Paper | code |
| :-----| :-----| :----- |
| NSVD/KDDW | [“Improving regularized singular value decomposition for collaborative filtering,” in Proceedings of KDD cup and workshop, vol. 2007.](https://zhangyk8.github.io/teaching/file_spring2018/Improving_regularized_singular_value_decomposition_for_collaborative_filtering.pdf) | [py](https://github.com/thunderock/item_recommendation) |
| SVD++/KDD | [“Factorization meets the neighborhood: a multifaceted collaborative filtering model,” in Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, 2008](https://dl.acm.org/doi/abs/10.1145/1401890.1401944) | [py](https://github.com/cheungdaven/recommendation) |
| SVDfeature/JMLR | [“Svdfeature: a toolkit for feature-based collaborative filtering,” The Journal of Machine Learning Research, 2012](https://www.jmlr.org/papers/volume13/chen12a/chen12a.pdf) | [toolkit](https://mloss.org/software/view/333/) |
| DELF/IJCAI | [“Delf: A dual-embedding based deep latent factor model for recommendation.” in IJCAI,](http://weiyu-cheng.com/Files/0462.pdf) | - |
| SLIM/ICDM | [“Slim: Sparse linear methods for top-n recommender systems,” in 2011 IEEE 11th international conference on data mining. IEEE, 2011, pp. 497–506.](https://ieeexplore.ieee.org/abstract/document/6137254) | [py](https://github.com/szktkfm/SLIM) |
| FISM/KDD | [ “Fism: factored item similarity models for top-n recommender systems,” in Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, 2013, pp. 659–667.](https://dl.acm.org/doi/abs/10.1145/2487575.2487589) | [tf](https://github.com/yushuai/FISM) |
| NAIS/TKDE | [ “Nais: Neural attentive item similarity model for recommendation,” IEEE Transactions on Knowledge and Data Engineering, vol. 30, no. 12, pp. 2354–2366, 2018](https://ieeexplore.ieee.org/abstract/document/8352808/) | [official(tf)](https://github.com/AaronHeee/Neural-Attentive-Item-Similarity-Model) |
| BiasSVD | [“Matrix factorization techniques for recommender systems,” Computer, vol. 42, no. 8, pp. 30–37, 2009.](https://ieeexplore.ieee.org/abstract/document/5197422) |[py](https://github.com/harshraj11584/Paper-Implementation-Matrix-Factorization-Recommender-Systems-Netflix)  |
| TimeSVD/KDD | [“Collaborative filtering with temporal dynamics,” in Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, 2009.](https://dl.acm.org/doi/abs/10.1145/1557019.1557072) | - |
| SGNS/Neurips | [“Neural word embedding as implicit matrix factorization,” Advances in neural information processing systems, vol. 27, 2014.](https://proceedings.neurips.cc/paper/2014/hash/feab05aa91085b7a8012516bc3533958-Abstract.html) | [py](https://github.com/a1da4/sppmi-svd) |
| ConvMF/Neurips | [ “Convolutional matrix factorization for document context-aware recommendation,” in Proceedings of the 10th ACM conference on recommender systems, 2016,pp. 233–240.](https://dl.acm.org/doi/abs/10.1145/2959100.2959165) | [torch](https://github.com/morinota/ConvMF) |
| FM/ICDM | [“Factorization machines,” in 2010 IEEE International conference on data mining. IEEE, 2010, pp. 995–1000.](https://ieeexplore.ieee.org/abstract/document/5694074/) | - |
| FFM/Recsys | [“Field-aware factorization machines for ctr prediction,” in Proceedings of the 10th ACM conference on recommender systems, 2016, pp. 43–50.](https://dl.acm.org/doi/abs/10.1145/2959100.2959134) | [tf](https://github.com/nnkkmto/field-aware-factorization-machines-tf2) |
| FNN/ECIR | [“Deep learning over multi-field categorical data,” in European conference on information retrieval. Springer, 2016, pp. 45–57.](https://link.springer.com/chapter/10.1007/978-3-319-30671-1_4) | [tf](https://github.com/nnkkmto/factorization-machine-supported-neural-network-tf2) |
| PNN/ICDM | [“Product-based neural networks for user response prediction,” in 2016 IEEE 16th International Conference on Data Mining (ICDM). IEEE, 2016, pp. 1149–1154.](https://ieeexplore.ieee.org/abstract/document/7837964/) | [tf](https://github.com/nnkkmto/product-based-neural-networks-tf2) |
| NeuMF/SIGIR | [“Neural factorization machines for sparse predictive analytics,” in Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval, 2017, pp. 355–364.](https://dl.acm.org/doi/abs/10.1145/3077136.3080777) | [torch](https://github.com/guoyang9/NFM-pyorch) |
| AFM | [“Attentional factorization machines: Learning the weight of feature interactions via attention networks”](https://arxiv.org/abs/1708.04617) | [offcial](https://github.com/hexiangnan/attentional_factorization_machine) |
| Wide & deep/DLRS | [“Wide & deep learning for recommender systems,” in Proceedings of the 1st workshop on deep learning for recommender systems, 2016, pp. 7–10](https://dl.acm.org/doi/abs/10.1145/2988450.2988454) | [offcial](https://github.com/hexiangnan/attentional_factorization_machine), [torch](https://github.com/shenweichen/DeepCTR-Torch) |
| DeepFM | [“DeepFM: a factorizationmachine based neural network for ctr prediction”,2017 ](https://arxiv.org/abs/1703.04247) | [torch](https://github.com/chenxijun1029/DeepFM_with_PyTorch),[tf](https://github.com/ChenglongChen/tensorflow-DeepFM) |
| DCN | [“Deep & cross network for ad click predictions,” in Proceedings of the ADKDD’17, 2017, pp. 1–7.](https://dl.acm.org/doi/abs/10.1145/3124749.3124754) | [torch](https://github.com/shenweichen/DeepCTR-Torch), [tf](https://github.com/shenweichen/DeepCTR) |
| xDeepFM/KDD | [“xdeepfm: Combining explicit and implicit feature interactions for recommender systems,” in Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining, 2018.](https://dl.acm.org/doi/abs/10.1145/3219819.3220023) | [official](https://github.com/Leavingseason/xDeepFM) |
| Autoint/CIKM | [“Autoint: Automatic feature interaction learning via self-attentive neural networks,” in Proceedings of the 28th ACM International Conference on Information and Knowledge Management, 2019](https://dl.acm.org/doi/abs/10.1145/3357384.3357925) | [official](https://github.com/DeepGraphLearning/RecommenderSystems) |






## Hash
| Name | Paper | code |
| :-----| :-----| :----- | 
| Hashing trick | [“Feature hashing for large scale multitask learning,” in Proceedings of the 26th annual international conference on machine learning, 2009, pp. 1113–1120.](https://dl.acm.org/doi/abs/10.1145/1553374.1553516) | [C++](https://github.com/prateekstark/feature-hashing) |
| Bloom embedding/Recsys | [“Getting deep recommenders fit: Bloom embeddings for sparse binary input/output networks,” in Proceedings of the Eleventh ACM Conference on Recommender Systems, 2017, pp. 279–287](https://dl.acm.org/doi/abs/10.1145/3109859.3109876) | - |
| Hash embeddings/Neurips | [“Hash embeddings for efficient word representations,” Advances in neural information processing systems, vol. 30, 2017.](https://proceedings.neurips.cc/paper/2017/hash/f0f6ba4b5e0000340312d33c212c3ae8-Abstract.html) | [tf](https://github.com/dsv77/hashembedding) |
| Hybrid hashing /Recsys| [“Model size reduction using frequency based double hashing for recommender systems,” in Fourteenth ACM Conference on Recommender Systems, 2020, pp. 521–526.](https://dl.acm.org/doi/abs/10.1145/3383313.3412227) | - |
|  Q-R trick/KDD | [“Compositional embeddings using complementary partitions for memory-efficient recommendation systems,” in Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020, pp. 165–175.](https://dl.acm.org/doi/abs/10.1145/3394486.3403059) | [official(torch)](https://github.com/facebookresearch/dlrm) |
| BH/CIKM | [“Binary code based hash embedding for web-scale applications,” in Proceedings of the 30th ACM International Conference on Information & Knowledge Management, 2021, pp. 3563–3567.](https://dl.acm.org/doi/abs/10.1145/3459637.3482065) | - |
| DHE/KDD | [“Learning to embed categorical features without embedding tables for recommendation,” arXiv preprint arXiv:2010.10784, 2020.](https://arxiv.org/abs/2010.10784) | - |



## AutoML
| Name |Paper | Remarks| Year | Proceeding | code |
| :-----| :-----| ------ |:-----| :----- | :----- |
| NIS/KDD |[Neural input search for large scale recommendation models](https://dl.acm.org/doi/abs/10.1145/3394486.3403288) | The first work of embedding dimension search (EDS) by reinforcement learning, where the controller directly selects the embedding size. | 2020 | KDD | - |
| ESAPN |[“Automated embedding size search in deep recommender systems” ](https://dl.acm.org/doi/abs/10.1145/3397271.3401436) | Similar to the NIS, the controller decides whether to increase or maintain the embedding size.| 2020 | SIGIR | [torch](https://github.com/zgahhblhc/ESAPN) |
| DARTS |[“Darts: Differentiable architecture search”](https://arxiv.org/abs/1806.09055) | The foundation of the gradient-based embedding size searching methods.| 2018 | ICLR | [torch](https://github.com/khanrc/pt.darts) |
| DNIS| [“Differentiable neural input search for recommender systems,” ](https://arxiv.org/abs/2006.04466) | To optimize the framework for EDS by gradient information, it introduces the soft selection layer, where it conducts a weighted summation of the candidate embedding size. | 2020 | | - |
| AutoSrh |[“AutoSrh: An Embedding Dimensionality Search Framework for Tabular Data Prediction”](https://ieeexplore.ieee.org/abstract/document/9807387) | It follows the pipeline of DNIS to make tabular data prediction. | 2021 | CIKM | - |
| AutoEMB | [“Autoemb: Automated embedding dimensionality search in streaming recommendations,”](https://ieeexplore.ieee.org/abstract/document/9679068) | To improve the hard selection in ESAPN, it also designed the soft selection layer as a weighted sum operation. | 2021 | ICDM | - |
| AutoDim |  ["Autodim: Field-aware embedding dimension searching recommender systems,"](https://ieeexplore.ieee.org/abstract/document/9679068) |It extends the input to various feature fields and employs Gumbel-softmax tricks as a soft selection layer.| 2021 | WWW | - |
| AMTL  |  ["Learning Effective and Efficient Embedding via an Adaptively-Masked Twins-based Layer"](https://dl.acm.org/doi/abs/10.1145/3459637.3482130) |It aims to learn a mask matrix to tailor the embedding size.| 2021 | CIKM | - |
| SSEDS | [“Single-shot embedding dimension search in a recommender system,” ](https://dl.acm.org/doi/abs/10.1145/3477495.3532060) | Considering the high training costs of AMTL, It learns the mask matrix by computing the saliency score.  | 2022 | SIGIR | - |
| RULE |["Learning elastic embeddings for customizing on-device recommenders,"](https://dl.acm.org/doi/abs/10.1145/3447548.3467220) | It introduces the evolutionary algorithms to conduct EDS.| 2021 | KDD | - |
| PEP |[“Learnable embedding sizes for recommender systems,” ](https://arxiv.org/abs/2101.07577) | Different from AMTL, PEP directly prune the embedding matrix by learning a threshold automatically.| 2021 | ICLR | [torch](https://github.com/ssui-liu/learnable-embed-sizes-for-RecSys) |
| ANT |[“Anchor & transform: Learning sparse embeddings for large vocabularies,” ](https://arxiv.org/abs/2003.08197) | It combines learnable anchor embedding matrices to form an optimal embedding matrix. | 2020 | ICLR | - |
| AutoDis |[“An embedding learning framework for numerical features in ctr prediction,”]() | Different from the hard selection of anchor embedding in ANT, it designs a differentiable automatic discretization network to execute a soft selection of meta-embeddings. | 2021 | KDD | [official](https://github.com/mindspore-ai/models/tree/master/research/recommend/autodis) |




## SSL
| Name | Paper | Remark | Year | Proceedings | code |
| :-----| :----- | ------ | ------ | :----- | ------ |
| SimCLR | [“A simple framework for contrastive learning of visual representations,” in International conference on machine learning. PMLR, 2020, pp. 1597–1607.](http://proceedings.mlr.press/v119/chen20j.html) | following SimCLR, it adopts the InfoNCE loss to maximize the agreement between positive samples and minimize the agreement between negative samples. | 2020 | ICML | [torch](https://github.com/sthalles/SimCLR) |
| SGL | [“Self-supervised graph learning for recommendation,” in Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval, 2021, pp. 726–735.](https://dl.acm.org/doi/abs/10.1145/3404835.3462862) | it is the first work utilizing contrastive learning for recommendation by designing an auxiliary task to supplement the recommendation model. | 2021 | SIGIR | [torch](https://github.com/AoiDragon/SGL_reproduction) |
| HHGR | [“Double-scale self-supervised hypergraph learning for group recommendation,” in Proceedings of the 30th ACM International Conference on Information & Knowledge Management, 2021, pp. 2557–2567.](https://dl.acm.org/doi/abs/10.1145/3459637.3482426) | it tackles the issue of distorted local structures due to random node dropout by propose a dual-scale node-dropping strategy. | 2021 | CIKM | [official(torch)](https://github.com/0411tony/hhgr) |
| CCDR/KDD | [“Contrastive cross-domain recommendation in matching,” in Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2022, pp. 4226–4236.](https://dl.acm.org/doi/abs/10.1145/3534678.3539125) | As a cross-domain recommendation method, it  transfers valuable node embeddings from a well-equipped source domain to a less-equipped target domain. | 2022 | KDD | [official(tf)](https://github.com/lqfarmer/ccdr) |
| PCRec | [“Pre-training graph neural network for cross domain recommendation,” in 2021 IEEE Third International Conference on Cognitive Machine Intelligence(CogMI). IEEE, 2021, pp. 140–145](https://ieeexplore.ieee.org/abstract/document/9750359) | it addresses cross-domain tasks by first pre-training on source domain and, then, fine-tuning on the target domain. | 2021 | CogMI | - |
| DCL | [“Contrastive learning for recommender system,” arXiv preprint arXiv:2101.01317, 2021.](https://arxiv.org/abs/2101.01317) | it challenges the assumption of interest in only sampled negative items by conducting subgraph sampling. | 2021 |  | - |
| CLS4Rec | [“Contrastive learning for sequential recommendation,” in 2022 IEEE 38th International Conference on Data Engineering (ICDE). IEEE, 2022, pp. 1259–1273.](https://ieeexplore.ieee.org/abstract/document/9835621) | it deals with the sequential recommendation task under contrastive learning paradigm. | 2022 | ICDE | [torch](https://github.com/RuihongQiu/DuoRec) |
| CoSeRec | [Z. Liu, Y. Chen, J. Li, P. S. Yu, J. McAuley, and C. Xiong, “Contrastive self-supervised sequential recommendation with robust augmentation,” arXiv preprint arXiv:2108.06479, 2021.](https://arxiv.org/abs/2108.06479) | Similar to CL4SRec, CoSeRec uses item substitution and insertion for data augmentation. | 2021 |  | [torch](https://github.com/YChen1993/CoSeRec) |
| Bert4rec | [“Bert4rec:Sequential recommendation with bidirectional encoder representations from transformer,” in Proceedings of the 28th ACM international conference on information and knowledge management, 2019, pp. 1441–1450.](https://dl.acm.org/doi/abs/10.1145/3357384.3357895) | it first utilizes the BERT for sequential recommendation, which applies item masking on the sample sequence and trains the model to predict the masked item. | 2019 | CIKM | [tf](https://github.com/FeiSun/BERT4Rec) |
| Unbert | [“Unbert: User-news matching bert for news recommendation.” in IJCAI, 2021, pp. 3356–3362.](https://www.ijcai.org/proceedings/2021/0462.pdf) | it extends the BERT model for news recommendation. | 2021 | IJCAI | - |
| UPRec | [“Uprec: User-aware pre-training for recommender systems,” ](https://arxiv.org/pdf/2102.10989.pdf) | it explores extending the BERT4Rec model to handle heterogeneous information such as user attributes and social networks. | 2021 |  | - |
| PeterRec | [“Parameter-efficient transfer from sequential behaviors for user modeling and recommendation,” ](https://dl.acm.org/doi/abs/10.1145/3397271.3401156) | it conducts learning-to-learn idea on cross-domain recommendation task. | 2020 | SIGIR | [official(tf)](https://github.com/fajieyuan/sigir2020_peterrec). |
| ShopperBERT | [“One4all user representation for recommender systems in ecommerce,” ](https://arxiv.org/abs/2106.00573) | it takes advantage of the rich user behaviors to pre-train the BERT model based on nine auxiliary tasks for the general embedding. | 2021 |  | - |
| G-BERT/IJCAI | [“Pre-training of graph augmented transformers for medication recommendation,” ](https://arxiv.org/abs/1906.00346) | it obtains sequences from graph data and then employs generative methods using the sequence data. | 2019 |  | [official(torch)](https://github.com/jshang123/G-Bert) |
| PMGT/ACM MM | [“Pre-training graph transformer with multimodal side information for recommendation,”](https://dl.acm.org/doi/abs/10.1145/3474085.3475709) | it focuses on multimodal recommendations, handling nodes with diverse information in a multimodal graph. | 2021 | ACMMM | - |
| PT-GNN/WSDM | [“Pre-training graph neural networks for cold-start users and items representation,”](https://dl.acm.org/doi/abs/10.1145/3437963.3441738) | it addresses cold-start nodes with few neighbors using a meta-learning approach. | 2021 | WSDM | [official(tf)](https://github.com/jerryhao66/Pretrain-Recsys) |

## Quantization
| Name             | Paper                                                        | Remark                                                       | Year | Proceeding | Code                                                         |
| :--------------- | :----------------------------------------------------------- | ------------------------------------------------------------ | ---- | ---------- | ------------------------------------------------------------ |
| DPR         | [Discrete personalized ranking for fast collaborative filtering from implicit feedback.](https://ojs.aaai.org/index.php/AAAI/article/view/10764) | DPR maps embeddings to binary codes by approximately optimizing AUC through a least-squares surrogate loss function,. | 2017 | AAAI       | -                                                            |
| DDL         | [Discrete deep learning for fast content-aware recommendation.](https://dl.acm.org/doi/abs/10.1145/3159652.3159688) | DDL uses a bag-of-words model to learn the embedding of the text of an item and minimizes the gap between it and the corresponding binary code. | 2018 | WSDM       | [MATLAB](https://github.com/yixianqianzy/ddl)                |
| PQ         | [ Product quantization for nearest neighbor search.](https://ieeexplore.ieee.org/abstract/document/5432202/) | PQ decomposes the high-dimensional feature space $\mathbb{R}^{D}$ into a cartesian product $\mathcal C={C}^{1} \times \cdots \times \mathcal{C}^{M} \in \mathbb{R}^{M \times K}$ of lower-dimensional subspaces. | 2010 | TPAMI      | [py](https://github.com/matsui528/nanopq)                    |
| OPQ        | [Optimized product quantization.](https://ieeexplore.ieee.org/abstract/document/6678503) | Optimized Product Quantization (OPQ) uses the rotation matrix $R \in \mathbb{R}^{M \times M}$ to optimize the decomposition of subspaces to reduce the correlation between subspaces. | 2013 | TPAMI      | [py](https://github.com/matsui528/nanopq)                    |
| AQ          | [Additive quantization for extreme vector compression.](https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Babenko_Additive_Quantization_for_2014_CVPR_paper.html) | Additive Quantization (AQ) \cite{babenko2014additive} adopts a different strategy from PQ, it directly assigns $M$ codebooks to the whole high-dimensional space, and the $M$ compressed vectors are summed to obtain the quantization embedding. | 2014 | CVPR       | -                                                            |
| DPQ         | [Differentiable product quantization for end-to-end embedding compression.](https://proceedings.mlr.press/v119/chen20l.html) | DPQ proposes softmax-based and centroid-based methods to minimize reconstruction loss approximately between raw embeddings and quantization embeddings. This approach leads to higher-quality quantization embeddings compared to unsupervised PQ algorithms. | 2020 | ICML       | [official(tf)](https://github.com/chentingpc/dpq_embedding_compression) |
| Lightrec     | [Lightrec: A memory and search-efficient recommender system.](https://dl.acm.org/doi/abs/10.1145/3366423.3380151) | LightRec enhances reconstruction loss with two functions, minimizing differences in user-item ratings pre- and post-quantization, as well as alterations in recommendation ranking. | 2020 | WWW        | [official(tf)](https://github.com/haoyu0408/LightRec)        |
| PQCF        | [Product quantized collaborative filtering.](https://ieeexplore.ieee.org/abstract/document/8950031) | Product Quantized Collaborative Filtering (PQCF) challenges separate user and item quantization using PQ. Misaligned coordinates make this suboptimal. PQCF minimizes rating prediction loss, moving from Euclidean to the inner product space. | 2020 | TKDE       | -                                                            |
| Distill-vq | [Distill-vq: Learning retrieval oriented vector quantization by distilling knowledge from dense embeddings.](https://dl.acm.org/doi/abs/10.1145/3477495.3531799) | Distill-VQ inspired by knowledge distillation, sets its objective as a similarity function measuring differences in relevance score distributions between teacher and student models (e.g., KL divergence). | 2020 | SIGIR      | [torch](https://github.com/staoxiao/libvq)                   |
| MOPQ       | [Matching-oriented product quantization for ad-hoc retrieval.](https://arxiv.org/pdf/2104.07858.pdf) | Matching-oriented Product Quantization (MoPQ) shows that better quantization reconstruction doesn't always mean better downstream performance. MoPQ improves accuracy through contrastive learning, modeling query-quantization matching via multinoulli process. | 2021 | EMNLP      | [torch](https://github.com/staoxiao/libvq)                   |
| xLightFM   | [xlightfm:Extremely memory-efficient factorization machine.](https://dl.acm.org/doi/abs/10.1145/3404835.3462941) | xLightFM assigns different codebooks to the features of different fields based on DQN. | 2021 | SIGIR      | [official(torch)](https://github.com/gangwJiang/x-LightFM)   |
| Online PQ   | [Online product quantization.](https://ieeexplore.ieee.org/abstract/document/8320306) | Online PQ maintains a sliding window for data processing, continuously applying K-means clustering for new codewords. | 2018 | TKDE       | -                                                            |
| Online OPQ  | [Online optimized product quantization.](https://ieeexplore.ieee.org/abstract/document/9338380) | Online OPQ extends to streaming data by solving the orthogonal procrustes problem to ensure subspace orthogonality during codebook updates. | 2020 | ICDM       | -                                                            |
| Online AQ    | [Online additive quantization.](https://dl.acm.org/doi/abs/10.1145/3447548.3467441) | Online AQ extends AQ, maintaining consistent objective functions. For streaming data adaptability, Online AQ derives codebook update strategies and related regret bounds via linear regression closed solutions and matrix inversion lemma. | 2021 | KDD        | -                                                            |

## Graph
| Name       | Paper                                                        | Remark                                                       | Year | Proceeding | Code                                                         |
| :--------- | :----------------------------------------------------------- | ------------------------------------------------------------ | ---- | ---------- | ------------------------------------------------------------ |
| Birank     | [Birank: Towards ranking on bipartite graphs.](https://ieeexplore.ieee.org/abstract/document/7572089) |——                                                              | 2016 | TKDE       | [py](https://github.com/mingboiz/social_network_analysis_birank) |
| Deepwalk   | [Deepwalk: Online learning of social representations.](https://dl.acm.org/doi/abs/10.1145/2623330.2623732) | Deepwalk is a classic graph representation learning method, which is designed based on random walk. | 2014 | KDD        | [nx](https://github.com/phanein/deepwalk)                    |
| APP        | [Scalable graph embedding for asymmetric proximity.](https://ojs.aaai.org/index.php/AAAI/article/view/10878) | Asymmetric Proximity Preserving (APP) graph embedding points out that in many downstream recommender system applications, the nodes in the graph do not have symmetry. | 2017 | AAAI       | [toolkit](https://github.com/benedekrozemberczki/karateclub) |
| LINE       | [Line: Large-scale information network embedding.](https://dl.acm.org/doi/abs/10.1145/2736277.2741093) | LINE considers both first-order similarity and second-order similarity of nodes | 2015 | WWW        | [official](https://github.com/tangjianpku/LINE)              |
| HyperSoRec | [Exploiting hyperbolic user and item representations with multiple aspects for social-aware recommendation.](https://dl.acm.org/doi/abs/10.1145/3463913) | HyperSoRec develops a hyperbolic mapping layer to map graph embedding in Euclidean space to hyperbolic space. | 2021 | TOIS       | -                                                            |
| HGCN       | [Hyperbolic graph convolutional neural networks](https://proceedings.neurips.cc/paper/2019/hash/0415740eaa4d9decbc8da001d3fd805f-Abstract.html) | HGCN show that tree graph embedding in hyperbolic space can achieve better performance than its counterpart in Euclidean space which may suffer from severe distortion. | 2019 | Neurips    | [torch](https://github.com/ZhangKaly/hgcn)                   |
| M2GRL      | [M2grl: A multitask multi-view graph representation learning framework for web-scale recommender systems.](https://dl.acm.org/doi/abs/10.1145/3394486.3403284) | M2GRL proposes to combine homogeneous graphs of multiple views to enhance sparse features and enrich node information. | 2020 | KDD        | -                                                            |
| DGENN      | [Dual graph enhanced embedding neural network for ctr prediction.](https://dl.acm.org/doi/abs/10.1145/3447548.3467384) | DGENN uses more than one view of homogeneous graphs to learn embeddings jointly. | 2021 | KDD        | -                                                            |
| Star-GCN   | [Star-gcn: Stacked and reconstructed graph convolutional networks for recommender systems.](https://arxiv.org/abs/1905.13129) | It simulates embedding new nodes with a masked vector, training the model to reconstruct embeddings. | 2019 | IJCAI      | [MXNET](https://github.com/jennyzhang0215/STAR-GCN)          |
| NGCF       | [Neural graph collaborative filtering.](https://dl.acm.org/doi/abs/10.1145/3331184.3331267) | NGCF~\cite{wang2019neural} addresses GC-MC's drawback by incorporating node features in its embeddings. | 2019 | SIGIR      | [code](https://paperswithcode.com/paper/neural-graph-collaborative-filtering#code) |
| UltraGCN   | [Ultragcn: ultra simplification of graph convolutional networks for recommendation.](https://dl.acm.org/doi/abs/10.1145/3459637.3482291) | UltraGCN removes the multi-layer messaging process by directly optimizing the cosine similarity of nodes and neighbors to capture the higher-order collaborative signals between users and items, and it uses a negative sampling strategy to avoid oversmoothing. | 2021 | CIKM       | [torch](https://github.com/xue-pai/UltraGCN)                 |
| CSE        | [Collaborative similarity embedding for recommender systems.](https://dl.acm.org/doi/abs/10.1145/3308558.3313493) | CSE suggests enhancing embeddings by incorporating higher-order similarities among nodes of the same type. | 2019 | WWW        | [code](https://paperswithcode.com/paper/collaborative-similarity-embedding-for#code) |
| BiNE       | [Learning vertex representations for bipartite networks.](https://ieeexplore.ieee.org/abstract/document/9039683) | ——                                                           | 2020 | TKDE       | [official(py)](https://github.com/clhchtcjj/BiNE)            |
| LightGCN   | [Lightgcn:Simplifying and powering graph convolution network for recommendation.](https://dl.acm.org/doi/abs/10.1145/3397271.3401063) | LightGCN argues that as the inputs of the user and item stem from ID embeddings lacking semantic information, there is no need for nonlinear transformations. | 2020 | SIGIR      | [official(torch)](https://github.com/gusye1234/LightGCN-PyTorch) |
| PGE        | [Learning graph-based embedding for time-aware product recommendation.](https://dl.acm.org/doi/abs/10.1145/3132847.3133060) | PGE considers temporal decay for item weights                | 2017 | CIKM       | -                                                            |
| GEM        | [Joint eventpartner recommendation in event-based social networks.](https://ieeexplore.ieee.org/abstract/document/8509309) | ——                                                           | 2018 | ICDE       | -                                                            |
| DiffNet    | [A neural influence diffusion model for social recommendation.](https://dl.acm.org/doi/abs/10.1145/3331184.3331214) | DiffNet tackles social and user-item networks separately.    | 2019 | SIGIR      | [official(tf)](https://github.com/PeiJieSun/diffnet)         |
| GraphRec   | [Graph neural networks for social recommendation.](https://dl.acm.org/doi/abs/10.1145/3308558.3313488) | GraphRec employs GAT to better model real-world social influence by weighing friends' influence based on the similarity of their initial embeddings. | 2019 | WWW        | [official(torch)](https://github.com/wenqifan03/GraphRec-WWW19) |
| Diffnet++  | [Diffnet++: A neural influence and interest diffusion network for social recommendation.](https://ieeexplore.ieee.org/abstract/document/9311623) | DiffNet++ builds upon DiffNet by introducing a unified framework that considers both social networks and user-item bipartite graphs. | 2020 | TKDE       | [official(tf)](https://github.com/PeiJieSun/diffnet)         |
| EGES       | [Billion-scale commodity embedding for e-commerce recommendation in alibaba.](https://dl.acm.org/doi/abs/10.1145/3219819.3219869) | EGES leverages related side information like brand to aggregate nodes' embeddings for obtaining item embeddings in heterogeneous graphs. | 2018 | KDD        | [torch](https://github.com/busesese/Word2Vec-with-side-information) |
| DANSER     | [Dual graph attention networks for deep latent representation of multifaceted social effects in recommender systems.](https://dl.acm.org/doi/abs/10.1145/3308558.3313442) | DANSER introduces a dual GAT to capture both static and dynamic influence, acknowledging that a user's impact on friends can vary based on items, leading to more realistic embeddings. | 2019 | WWW        | [official(tf)](https://github.com/qitianwu/DANSER-WWW-19)    |
| TransGRec  | [Learning to transfer graph embeddings for inductive graph based recommendation.](https://dl.acm.org/doi/abs/10.1145/3397271.3401145) | ——                                                           | 2020 | SIGIR      | -                                                            |
| GHL        | [Gated heterogeneous graph representation learning for shop search in e-commerce.](https://dl.acm.org/doi/abs/10.1145/3340531.3412087) | ——                                                           | 2020 | CIKM       | -                                                            |
| TransE     | [Translating embeddings for modeling multi-relational data.](https://proceedings.neurips.cc/paper/2013/hash/1cecc7a77928ca8133fa24680a88d2f9-Abstract.html) | Translation-based models used to learn entity embeddings。   | 2013 | Neurips    | [code](https://github.com/HeenaRajan/Translating_Embedding_for_Modelling_Multi-Relational_Data) |
| TransH     | [Knowledge graph embedding by translating on hyperplanes.](https://ojs.aaai.org/index.php/AAAI/article/view/8870) | Translation-based models used to learn entity embeddings。   | 2014 | AAAI       | [code](https://github.com/yangyucheng000/transX)             |
| TransR     | [Learning entity and relation embeddings for knowledge graph completion.](https://ojs.aaai.org/index.php/AAAI/article/view/9491) | Translation-based models used to learn entity embeddings。   | 2015 | AAAI       | [code](https://github.com/yanwii/Cluster-based-TransR)       |
| KGAT       | [Kgat: Knowledge graph attention network for recommendation.](https://dl.acm.org/doi/abs/10.1145/3292500.3330989) | KGAT apply GAT to produce higher-quality embeddings in knowledge graph. | 2019 | KDD        | [tf](https://github.com/xiangwang1223/knowledge_graph_attention_network) |
| KGIN       | [Learning intents behind interactions with knowledge graph for recommendation.](https://dl.acm.org/doi/abs/10.1145/3442381.3450133) | KGCN apply GCN and GAT respectively to produce higher-quality embeddings. | 2021 | WWW        | [torch](https://github.com/huangtinglin/Knowledge_Graph_based_Intent_Network) |
| IHGNN      | [Ihgnn: Interactive hypergraph neural network for personalized product search.](https://dl.acm.org/doi/abs/10.1145/3485447.3511954) | IHGNN enhances node embeddings through hypergraphs, revealing higher-order interaction patterns in user query histories | 2022 | WWW        | [torch](https://github.com/CDboyOne/IHGNN)                   |
| HGNN       | [Hypergraph neural networks](https://ojs.aaai.org/index.php/AAAI/article/view/4235) | Hypergraph GNNs                                              | 2019 | AAAI       | [torch](https://github.com/iMoonLab/HGNN)                    |
| HyperGCN   | [Hypergcn: A new method for training graph convolutional networks on hypergraphs.](https://proceedings.neurips.cc/paper/2019/hash/1efa39bcaec6f3900149160693694536-Abstract.html) | Hypergraph GNNs                                              | 2019 | Neurips    | [torch](https://github.com/malllabiisc/HyperGCN)             |
| HyperGroup | [Hierarchical hyperedge embedding-based representation learning for group recommendation.](https://dl.acm.org/doi/abs/10.1145/3457949) | HyperGroup targets group recommendation, addressing potential misalignment between user and group preferences by representing groups as hyperedges. | 2021 | TOIS       | -                                                            |
| HEMR       | [Music recommendation via hypergraph embedding.](https://ieeexplore.ieee.org/document/9709542) | HEMR focuses on music recommendation using hypergraph embeddings. It employs hyperedge-level random walks, followed by skip-gram for node embedding learning. | 2022 | TNNLS      | [code](https://github.com/valeriolagatta/HEMR)               |
| DyGNN      | [Streaming graph neural networks.](https://dl.acm.org/doi/abs/10.1145/3397271.3401092) | A method for learning dynamic graph representation.          | 2020 | SIGIR      | [torch](https://github.com/alge24/DyGNN)A                    |
| ROLAND     | [Roland: graph learning framework for dynamic graphs.](https://dl.acm.org/doi/10.1145/3534678.3539300) | A method for learning dynamic graph representation.          | 2022 | KDD        | [code](https://github.com/andjelatodorovich/roland-river)    |
| GEAR       | [Learning fair node representations with graph counterfactual fairness.](https://dl.acm.org/doi/10.1145/3488560.3498391) | A method for learning representation of fair graphs.         | 2022 | WSDM       | -                                                            |







